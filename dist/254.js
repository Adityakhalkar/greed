"use strict";(this.webpackChunkGreed=this.webpackChunkGreed||[]).push([[254],{254:(e,n,t)=>{function s(){return'# GreedJS WebGPU PyTorch Runtime\n# Pure Python PyTorch implementation with direct WebGPU acceleration\n# Users write: import torch; x = torch.tensor([1,2,3]) \n# Backend: All operations run on WebGPU compute shaders\n\nimport js\nimport sys\nfrom typing import List, Optional, Union, Tuple\nimport array\n\nclass WebGPUDevice:\n    """Device abstraction for WebGPU operations"""\n    def __init__(self, device_type: str = \'webgpu\'):\n        self.type = device_type\n        # Safe WebGPU availability check with fallbacks\n        try:\n            self.is_available = (\n                hasattr(js, \'navigator\') and \n                hasattr(js.navigator, \'gpu\') and \n                js.navigator.gpu is not None and\n                js.navigator.gpu is not js.undefined\n            )\n        except (AttributeError, NameError):\n            # Fallback: assume WebGPU available, let greed instance handle detection\n            self.is_available = True\n    \n    def __str__(self):\n        return f"device(type=\'{self.type}\')"\n    \n    def __repr__(self):\n        return self.__str__()\n\n# Global device instances\ncuda = WebGPUDevice(\'webgpu\')  # For PyTorch compatibility (cuda -> webgpu)\nwebgpu = WebGPUDevice(\'webgpu\')\n\nclass WebGPUTensor:\n    """Pure WebGPU PyTorch-compatible tensor implementation"""\n    \n    def __init__(self, data, dtype=None, device=None, requires_grad=False):\n        self.device = device or webgpu\n        self.requires_grad = requires_grad\n        self.grad = None\n        self.grad_fn = None\n        \n        # Convert input data to appropriate format\n        if isinstance(data, WebGPUTensor):\n            self._webgpu_id = data._webgpu_id\n            self.shape = data.shape\n            self.dtype = data.dtype\n        elif isinstance(data, (list, tuple)):\n            # Create WebGPU tensor from Python list/tuple\n            self.shape = self._infer_shape(data)\n            self.dtype = dtype or self._infer_dtype(data)\n            self._webgpu_id = self._create_webgpu_tensor(data)\n        else:\n            # Handle scalar or other types\n            if not isinstance(data, (list, tuple)):\n                data = [data]\n            self.shape = [len(data)] if isinstance(data, (list, tuple)) else [1]\n            self.dtype = dtype or self._infer_dtype(data)\n            self._webgpu_id = self._create_webgpu_tensor(data)\n    \n    def _create_webgpu_tensor(self, data):\n        """Create WebGPU tensor through JavaScript bridge"""\n        # Flatten the data for WebGPU\n        flat_data = self._flatten_data(data)\n        \n        # Call JavaScript tensor bridge to create WebGPU tensor\n        result = js.greedInstance.tensorBridge.createWebGPUTensor(\n            flat_data,\n            self.shape, \n            self.dtype,\n            str(self.device)\n        )\n        \n        return result.id\n    \n    def _flatten_data(self, data):\n        """Recursively flatten nested lists/tuples to flat array"""\n        def flatten(lst):\n            result = []\n            for item in lst:\n                if isinstance(item, (list, tuple)):\n                    result.extend(flatten(item))\n                else:\n                    result.append(float(item))\n            return result\n        \n        return flatten(data)\n    \n    def _infer_shape(self, data):\n        """Infer tensor shape from nested lists/tuples"""\n        if not isinstance(data, (list, tuple)):\n            return []\n        \n        shape = []\n        current = data\n        while isinstance(current, (list, tuple)) and len(current) > 0:\n            shape.append(len(current))\n            if isinstance(current[0], (list, tuple)):\n                current = current[0]\n            else:\n                break\n        \n        return shape\n    \n    def _infer_dtype(self, data):\n        """Infer PyTorch dtype from data"""\n        flat = self._flatten_data(data) if isinstance(data, (list, tuple)) else [data]\n        \n        if not flat:\n            return \'float32\'\n        \n        sample = flat[0]\n        if isinstance(sample, bool):\n            return \'bool\'\n        elif isinstance(sample, int):\n            return \'int64\'\n        elif isinstance(sample, float):\n            return \'float32\'\n        else:\n            return \'float32\'\n    \n    # ===== TENSOR OPERATIONS =====\n    \n    def add(self, other):\n        """Element-wise addition using WebGPU"""\n        other_tensor = other if isinstance(other, WebGPUTensor) else WebGPUTensor(other)\n        \n        # Call WebGPU add shader through JavaScript\n        result_id = js.greedInstance.tensorBridge.executeOperation(\n            \'add\',\n            self._webgpu_id,\n            other_tensor._webgpu_id,\n            {\'shape\': self.shape, \'dtype\': self.dtype}\n        )\n        \n        result = WebGPUTensor.__new__(WebGPUTensor)\n        result._webgpu_id = result_id\n        result.shape = self.shape\n        result.dtype = self.dtype\n        result.device = self.device\n        result.requires_grad = self.requires_grad or other_tensor.requires_grad\n        \n        return result\n    \n    def __add__(self, other):\n        return self.add(other)\n    \n    def sub(self, other):\n        """Element-wise subtraction using WebGPU"""\n        other_tensor = other if isinstance(other, WebGPUTensor) else WebGPUTensor(other)\n        \n        result_id = js.greedInstance.tensorBridge.executeOperation(\n            \'sub\',\n            self._webgpu_id,\n            other_tensor._webgpu_id,\n            {\'shape\': self.shape, \'dtype\': self.dtype}\n        )\n        \n        result = WebGPUTensor.__new__(WebGPUTensor)\n        result._webgpu_id = result_id\n        result.shape = self.shape\n        result.dtype = self.dtype\n        result.device = self.device\n        result.requires_grad = self.requires_grad or other_tensor.requires_grad\n        \n        return result\n    \n    def __sub__(self, other):\n        return self.sub(other)\n    \n    def mul(self, other):\n        """Element-wise multiplication using WebGPU"""\n        other_tensor = other if isinstance(other, WebGPUTensor) else WebGPUTensor(other)\n        \n        result_id = js.greedInstance.tensorBridge.executeOperation(\n            \'mul\',\n            self._webgpu_id,\n            other_tensor._webgpu_id,\n            {\'shape\': self.shape, \'dtype\': self.dtype}\n        )\n        \n        result = WebGPUTensor.__new__(WebGPUTensor)\n        result._webgpu_id = result_id\n        result.shape = self.shape\n        result.dtype = self.dtype  \n        result.device = self.device\n        result.requires_grad = self.requires_grad or other_tensor.requires_grad\n        \n        return result\n    \n    def __mul__(self, other):\n        return self.mul(other)\n    \n    def matmul(self, other):\n        """Matrix multiplication using WebGPU"""\n        other_tensor = other if isinstance(other, WebGPUTensor) else WebGPUTensor(other)\n        \n        # Calculate output shape for matrix multiplication\n        if len(self.shape) == 2 and len(other_tensor.shape) == 2:\n            output_shape = [self.shape[0], other_tensor.shape[1]]\n        else:\n            raise ValueError(f"matmul: incompatible shapes {self.shape} and {other_tensor.shape}")\n        \n        result_id = js.greedInstance.tensorBridge.executeOperation(\n            \'matmul\',\n            self._webgpu_id,\n            other_tensor._webgpu_id,\n            {\'shape\': output_shape, \'dtype\': self.dtype}\n        )\n        \n        result = WebGPUTensor.__new__(WebGPUTensor)\n        result._webgpu_id = result_id\n        result.shape = output_shape\n        result.dtype = self.dtype\n        result.device = self.device\n        result.requires_grad = self.requires_grad or other_tensor.requires_grad\n        \n        return result\n    \n    def __matmul__(self, other):\n        return self.matmul(other)\n    \n    def sum(self, dim=None, keepdim=False):\n        """Sum reduction using WebGPU"""\n        result_id = js.greedInstance.tensorBridge.executeOperation(\n            \'sum\',\n            self._webgpu_id,\n            None,\n            {\n                \'shape\': self.shape,\n                \'dtype\': self.dtype,\n                \'dim\': dim,\n                \'keepdim\': keepdim\n            }\n        )\n        \n        # Calculate output shape based on reduction\n        if dim is None:\n            output_shape = [1] if keepdim else []\n        else:\n            output_shape = self.shape.copy()\n            if keepdim:\n                output_shape[dim] = 1\n            else:\n                output_shape.pop(dim)\n        \n        result = WebGPUTensor.__new__(WebGPUTensor)\n        result._webgpu_id = result_id\n        result.shape = output_shape\n        result.dtype = self.dtype\n        result.device = self.device\n        result.requires_grad = self.requires_grad\n        \n        return result\n    \n    def mean(self, dim=None, keepdim=False):\n        """Mean reduction using WebGPU"""\n        result_id = js.greedInstance.tensorBridge.executeOperation(\n            \'mean\',\n            self._webgpu_id,\n            None,\n            {\n                \'shape\': self.shape,\n                \'dtype\': self.dtype,\n                \'dim\': dim,\n                \'keepdim\': keepdim\n            }\n        )\n        \n        # Calculate output shape\n        if dim is None:\n            output_shape = [1] if keepdim else []\n        else:\n            output_shape = self.shape.copy()\n            if keepdim:\n                output_shape[dim] = 1\n            else:\n                output_shape.pop(dim)\n        \n        result = WebGPUTensor.__new__(WebGPUTensor)\n        result._webgpu_id = result_id\n        result.shape = output_shape\n        result.dtype = self.dtype\n        result.device = self.device\n        result.requires_grad = self.requires_grad\n        \n        return result\n    \n    def relu(self):\n        """ReLU activation using WebGPU"""\n        result_id = js.greedInstance.tensorBridge.executeOperation(\n            \'relu\',\n            self._webgpu_id,\n            None,\n            {\'shape\': self.shape, \'dtype\': self.dtype}\n        )\n        \n        result = WebGPUTensor.__new__(WebGPUTensor)\n        result._webgpu_id = result_id\n        result.shape = self.shape\n        result.dtype = self.dtype\n        result.device = self.device\n        result.requires_grad = self.requires_grad\n        \n        return result\n    \n    def backward(self, gradient=None):\n        """Backpropagation using WebGPU autograd"""\n        if not self.requires_grad:\n            return\n        \n        if gradient is None:\n            # Create gradient of ones with same shape\n            gradient = ones_like(self)\n        \n        # Execute backward pass through JavaScript autograd\n        js.greedInstance.tensorBridge.executeBackward(\n            self._webgpu_id,\n            gradient._webgpu_id if isinstance(gradient, WebGPUTensor) else gradient\n        )\n    \n    # ===== TENSOR PROPERTIES =====\n    \n    def item(self):\n        """Get scalar value from single-element tensor"""\n        if len(self.shape) != 0 and self.shape != [1]:\n            raise ValueError("item() only works on tensors with one element")\n        \n        # Get data from WebGPU tensor\n        data = js.greedInstance.tensorBridge.getTensorData(self._webgpu_id)\n        return data[0]\n    \n    def numpy(self):\n        """Convert to numpy array (for debugging/compatibility)"""\n        data = js.greedInstance.tensorBridge.getTensorData(self._webgpu_id)\n        # Note: In real implementation, this would return actual numpy array\n        return list(data)\n    \n    def size(self, dim=None):\n        """Get tensor size"""\n        if dim is None:\n            return self.shape\n        else:\n            return self.shape[dim]\n    \n    def numel(self):\n        """Total number of elements"""\n        result = 1\n        for dim in self.shape:\n            result *= dim\n        return result\n    \n    def __repr__(self):\n        """String representation"""\n        data_str = str(self.numpy()[:10])  # Show first 10 elements\n        if self.numel() > 10:\n            data_str = data_str[:-1] + ", ...]"\n        \n        return f"tensor({data_str}, device=\'{self.device}\', dtype={self.dtype})"\n    \n    def __str__(self):\n        return self.__repr__()\n\n# ===== TENSOR CREATION FUNCTIONS =====\n\ndef tensor(data, dtype=None, device=None, requires_grad=False):\n    """Create tensor from data"""\n    return WebGPUTensor(data, dtype=dtype, device=device, requires_grad=requires_grad)\n\ndef zeros(size, dtype=\'float32\', device=None, requires_grad=False):\n    """Create zero tensor using WebGPU zeros shader"""\n    shape = size if isinstance(size, (list, tuple)) else [size]\n    \n    result_id = js.greedInstance.tensorBridge.executeCreationOperation(\n        \'zeros\',\n        {\'shape\': shape, \'dtype\': dtype, \'device\': str(device or webgpu)}\n    )\n    \n    result = WebGPUTensor.__new__(WebGPUTensor)\n    result._webgpu_id = result_id\n    result.shape = shape\n    result.dtype = dtype\n    result.device = device or webgpu\n    result.requires_grad = requires_grad\n    \n    return result\n\ndef ones(size, dtype=\'float32\', device=None, requires_grad=False):\n    """Create ones tensor using WebGPU"""\n    shape = size if isinstance(size, (list, tuple)) else [size]\n    \n    result_id = js.greedInstance.tensorBridge.executeCreationOperation(\n        \'ones\',\n        {\'shape\': shape, \'dtype\': dtype, \'device\': str(device or webgpu)}\n    )\n    \n    result = WebGPUTensor.__new__(WebGPUTensor)\n    result._webgpu_id = result_id\n    result.shape = shape\n    result.dtype = dtype\n    result.device = device or webgpu\n    result.requires_grad = requires_grad\n    \n    return result\n\ndef randn(size, dtype=\'float32\', device=None, requires_grad=False):\n    """Create random normal tensor using WebGPU"""\n    shape = size if isinstance(size, (list, tuple)) else [size]\n    \n    result_id = js.greedInstance.tensorBridge.executeCreationOperation(\n        \'randn\',\n        {\'shape\': shape, \'dtype\': dtype, \'device\': str(device or webgpu)}\n    )\n    \n    result = WebGPUTensor.__new__(WebGPUTensor)\n    result._webgpu_id = result_id\n    result.shape = shape\n    result.dtype = dtype\n    result.device = device or webgpu\n    result.requires_grad = requires_grad\n    \n    return result\n\ndef ones_like(tensor_like):\n    """Create ones tensor with same shape as input"""\n    return ones(tensor_like.shape, dtype=tensor_like.dtype, device=tensor_like.device)\n\ndef zeros_like(tensor_like):\n    """Create zeros tensor with same shape as input"""\n    return zeros(tensor_like.shape, dtype=tensor_like.dtype, device=tensor_like.device)\n\n# ===== FUNCTIONAL OPERATIONS =====\n\ndef relu(input):\n    """ReLU activation function"""\n    return input.relu()\n\ndef matmul(input, other):\n    """Matrix multiplication"""\n    return input.matmul(other)\n\ndef add(input, other):\n    """Element-wise addition"""\n    return input.add(other)\n\n# ===== NEURAL NETWORK MODULE =====\n\nclass Module:\n    """Base class for neural network modules"""\n    \n    def __init__(self):\n        self._parameters = {}\n        self.training = True\n    \n    def __call__(self, *args, **kwargs):\n        return self.forward(*args, **kwargs)\n    \n    def forward(self, *args, **kwargs):\n        raise NotImplementedError("Subclasses must implement forward()")\n    \n    def parameters(self):\n        """Return all parameters"""\n        params = []\n        for param in self._parameters.values():\n            params.append(param)\n        return params\n    \n    def train(self, mode=True):\n        """Set training mode"""\n        self.training = mode\n        return self\n    \n    def eval(self):\n        """Set evaluation mode"""\n        return self.train(False)\n\nclass Linear(Module):\n    """Linear (fully connected) layer with WebGPU acceleration"""\n    \n    def __init__(self, in_features, out_features, bias=True):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.use_bias = bias\n        \n        # Initialize weights with Kaiming initialization\n        self.weight = randn([out_features, in_features], requires_grad=True)\n        if bias:\n            self.bias = zeros([out_features], requires_grad=True)\n        else:\n            self.bias = None\n        \n        self._parameters[\'weight\'] = self.weight\n        if bias:\n            self._parameters[\'bias\'] = self.bias\n    \n    def forward(self, input):\n        """Forward pass: input @ weight.T + bias"""\n        # WebGPU linear layer operation\n        result_id = js.greedInstance.tensorBridge.executeOperation(\n            \'linear\',\n            input._webgpu_id,\n            self.weight._webgpu_id,\n            {\n                \'bias_id\': self.bias._webgpu_id if self.bias else None,\n                \'in_features\': self.in_features,\n                \'out_features\': self.out_features,\n                \'input_shape\': input.shape,\n                \'dtype\': input.dtype\n            }\n        )\n        \n        output_shape = input.shape[:-1] + [self.out_features]\n        \n        result = WebGPUTensor.__new__(WebGPUTensor)\n        result._webgpu_id = result_id\n        result.shape = output_shape\n        result.dtype = input.dtype\n        result.device = input.device\n        result.requires_grad = True\n        \n        return result\n\nclass ReLU(Module):\n    """ReLU activation function"""\n    \n    def forward(self, input):\n        return relu(input)\n\nclass Sequential(Module):\n    """Sequential container for modules"""\n    \n    def __init__(self, *modules):\n        super().__init__()\n        self.modules_list = list(modules)\n        \n        # Collect parameters from all modules\n        for i, module in enumerate(self.modules_list):\n            if hasattr(module, \'_parameters\'):\n                for name, param in module._parameters.items():\n                    self._parameters[f\'module_{i}_{name}\'] = param\n    \n    def forward(self, input):\n        for module in self.modules_list:\n            input = module(input)\n        return input\n\n# Create nn submodule for PyTorch compatibility\nclass nn:\n    Module = Module\n    Linear = Linear\n    ReLU = ReLU\n    Sequential = Sequential\n\n# ===== LOSS FUNCTIONS =====\n\nclass MSELoss(Module):\n    """Mean Squared Error Loss"""\n    \n    def forward(self, input, target):\n        # MSE = mean((input - target)^2)\n        diff = input.sub(target)\n        squared = diff.mul(diff)\n        return squared.mean()\n\nclass CrossEntropyLoss(Module):\n    """Cross Entropy Loss"""\n    \n    def forward(self, input, target):\n        # Implement cross entropy using WebGPU\n        result_id = js.greedInstance.tensorBridge.executeOperation(\n            \'cross_entropy\',\n            input._webgpu_id,\n            target._webgpu_id,\n            {\'input_shape\': input.shape, \'dtype\': input.dtype}\n        )\n        \n        result = WebGPUTensor.__new__(WebGPUTensor)\n        result._webgpu_id = result_id\n        result.shape = []  # Scalar loss\n        result.dtype = input.dtype\n        result.device = input.device\n        result.requires_grad = True\n        \n        return result\n\n# Add loss functions to nn\nnn.MSELoss = MSELoss\nnn.CrossEntropyLoss = CrossEntropyLoss\n\n# ===== DEVICE UTILITIES =====\n\ndef is_available():\n    """Check if WebGPU is available"""\n    try:\n        return (\n            hasattr(js, \'navigator\') and \n            hasattr(js.navigator, \'gpu\') and \n            js.navigator.gpu is not None and\n            js.navigator.gpu is not js.undefined\n        )\n    except (AttributeError, NameError):\n        # Fallback: assume WebGPU available, let greed instance handle detection\n        return True\n\n# Make cuda point to webgpu for PyTorch compatibility\nclass cuda_module:\n    @staticmethod\n    def is_available():\n        return is_available()\n\ncuda = cuda_module()\n\n# ===== TORCH MODULE CREATION =====\n\n# Create torch module namespace for PyTorch compatibility\nclass TorchModule:\n    """Main torch module containing all PyTorch functionality"""\n    \n    # Tensor creation functions\n    tensor = tensor\n    zeros = zeros\n    ones = ones\n    randn = randn\n    ones_like = ones_like\n    zeros_like = zeros_like\n    \n    # Functional operations\n    add = add\n    matmul = matmul\n    relu = relu\n    \n    # Neural network module\n    nn = nn\n    \n    # Device utilities\n    cuda = cuda\n    is_available = is_available\n    \n    # Tensor types\n    Tensor = WebGPUTensor\n    \n    # Additional PyTorch compatibility functions\n    def sum(self, input, dim=None, keepdim=False):\n        """Global sum function"""\n        return input.sum(dim=dim, keepdim=keepdim)\n    \n    def mean(self, input, dim=None, keepdim=False):\n        """Global mean function"""\n        return input.mean(dim=dim, keepdim=keepdim)\n\n# Create the torch instance\ntorch = TorchModule()\n\n# Register torch module properly in Python module system\nsys.modules[\'torch\'] = torch\nsys.modules[\'torch.nn\'] = torch.nn\nsys.modules[\'torch.cuda\'] = torch.cuda\n\n# Also make torch available in globals for direct access\nglobals()[\'torch\'] = torch\n\n# Export torch module interface  \n__all__ = [\n    \'torch\',  # Main torch module\n    \'tensor\', \'zeros\', \'ones\', \'randn\', \'ones_like\', \'zeros_like\',\n    \'relu\', \'matmul\', \'add\',\n    \'nn\', \'cuda\', \'is_available\',\n    \'WebGPUTensor\', \'WebGPUDevice\'\n]'}t.d(n,{createWebGPUPyTorchRuntime:()=>s})}}]);